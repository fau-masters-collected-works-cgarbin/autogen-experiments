{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential chat example with math operator agents\n",
    "\n",
    "From the [AutoGen _conversation patterns_ tutorial](https://microsoft.github.io/autogen/docs/tutorial/conversation-patterns#sequential-chats).\n",
    "\n",
    "This is an example of a _sequential chat_:\n",
    "\n",
    "> [A] sequence of chats between two agents, chained together by a mechanism called carryover, which brings the summary of the previous chat to the context of the next chat.\n",
    "\n",
    "This examples uses a sequence of agents to get to a new number, given a starting number. For example, starting at 1, get to 13.\n",
    "\n",
    "The agents are math operators and the carryover is the result of the previous operation.\n",
    "\n",
    "See annotations in the cells for items to experiment with and other observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM setup\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoGen setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example is highly sensitive to the model you use\n",
    "# It worked only with gpt-4 for me\n",
    "# Change to gpt-3.5-turbo or gpt-4o to see different results\n",
    "MODEL = \"gpt-4\"\n",
    "\n",
    "CONFIG = [{\"model\": MODEL, \"api_key\": OPENAI_API_KEY}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agents\n",
    "\n",
    "The `number_agent` comes up with the initial and final number. The operator agents are math operators that can apply one operation at a time to the nummber they are given, resulting in a new number that is passed on to the next operator agent.\n",
    "\n",
    "AutoGen will select the sequence of agents to get to the final number.\n",
    "\n",
    "The `description` field is the important piece here: it is used to decide which agent to use next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import ConversableAgent\n",
    "\n",
    "# The Number Agent always returns the same numbers.\n",
    "number_agent = ConversableAgent(\n",
    "    name=\"Number_Agent\",\n",
    "    description=\"Return the numbers given.\",\n",
    "    system_message=\"You return me the numbers I give you, one number each line.\",\n",
    "    llm_config={\"config_list\": CONFIG},\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "# The Adder Agent adds 1 to each number it receives.\n",
    "adder_agent = ConversableAgent(\n",
    "    name=\"Adder_Agent\",\n",
    "    description=\"Add 1 to each input number.\",\n",
    "    system_message=\"You add 1 to each number I give you and return me the new numbers, one number each line.\",\n",
    "    llm_config={\"config_list\": CONFIG},\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "# The Multiplier Agent multiplies each number it receives by 2.\n",
    "multiplier_agent = ConversableAgent(\n",
    "    name=\"Multiplier_Agent\",\n",
    "    description=\"Multiply each input number by 2.\",\n",
    "    system_message=\"You multiply each number I give you by 2 and return me the new numbers, one number each line.\",\n",
    "    llm_config={\"config_list\": CONFIG},\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "# The Subtracter Agent subtracts 1 from each number it receives.\n",
    "subtracter_agent = ConversableAgent(\n",
    "    name=\"Subtracter_Agent\",\n",
    "    description=\"Subtract 1 from each input number.\",\n",
    "    system_message=\"You subtract 1 from each number I give you and return me the new numbers, one number each line.\",\n",
    "    llm_config={\"config_list\": CONFIG},\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "# The Divider Agent divides each number it receives by 2.\n",
    "divider_agent = ConversableAgent(\n",
    "    name=\"Divider_Agent\",\n",
    "    description=\"Divide each input number by 2.\",\n",
    "    system_message=\"You divide each number I give you by 2 and return me the new numbers, one number each line.\",\n",
    "    llm_config={\"config_list\": CONFIG},\n",
    "    human_input_mode=\"NEVER\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent collaboration through a group chat\n",
    "\n",
    "The agents are part of a group chat:\n",
    "\n",
    "> The core idea of group chat is that all agents contribute to a single conversation thread and share the same context. This is useful for tasks that require collaboration among multiple agents.\n",
    "\n",
    "\"Collaboration\" in this context means that one agent is select to perform an action. The result of this task is passed to the next agent selected, and so on until the goal is reached.\n",
    "\n",
    "The secret sauce is the agent selection process. We are using `auto` here, which means that AutoGen will select the agents to use.\n",
    "\n",
    "This is what happens behind the scenes (from `_auto_select_speaker` in AutoGen's `groupchat.py`):\n",
    "\n",
    "> 1. Create a two-agent chat with a speaker selector agent and a speaker validator agent, like a nested chat\n",
    "> 2. Inject the group messages into the new chat\n",
    "> 3. Run the two-agent chat, evaluating the result of response from the speaker selector agent:\n",
    ">    - If a single agent is provided then we return it and finish. If not, we add an additional message to this nested chat in an attempt to guide the LLM to a single agent response\n",
    "> 4. Chat continues until a single agent is nominated or there are no more attempts left\n",
    "> 5. If we run out of turns and no single agent can be determined, the next speaker in the list of agents is returned\n",
    "\n",
    "In other words, AutoGen uses the LLM to select an agent. This process can consume a lot of tokens. We will see how many at the end of the notebook.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import GroupChat\n",
    "\n",
    "# This selection method uses the agent descriptions to select the agent\n",
    "SPEAKER_SELECTION_METHOD = \"auto\"\n",
    "\n",
    "group_chat = GroupChat(\n",
    "    agents=[adder_agent, multiplier_agent, subtracter_agent, divider_agent, number_agent],\n",
    "    speaker_selection_method=\"auto\",\n",
    "    messages=[],\n",
    "    max_round=6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import GroupChatManager\n",
    "\n",
    "group_chat_manager = GroupChatManager(\n",
    "    groupchat=group_chat,\n",
    "    llm_config={\"config_list\": CONFIG},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the example\n",
    "\n",
    "Now that everything is set up, we can run the example. It wil log each step of the conversation.\n",
    "\n",
    "Note that [AutoGen can cache LLM requests and responses](https://microsoft.github.io/autogen/docs/topics/llm-caching). If the examples runs very fast, it likely means the results have been cached and we are not going to the LLM for each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mNumber_Agent\u001b[0m (to chat_manager):\n",
      "\n",
      "My number is 3, I want to turn it into 13.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Multiplier_Agent\n",
      "\u001b[0m\n",
      "\u001b[33mMultiplier_Agent\u001b[0m (to chat_manager):\n",
      "\n",
      "6\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Multiplier_Agent\n",
      "\u001b[0m\n",
      "\u001b[33mMultiplier_Agent\u001b[0m (to chat_manager):\n",
      "\n",
      "26\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Subtracter_Agent\n",
      "\u001b[0m\n",
      "\u001b[33mSubtracter_Agent\u001b[0m (to chat_manager):\n",
      "\n",
      "5\n",
      "25\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Subtracter_Agent\n",
      "\u001b[0m\n",
      "\u001b[33mSubtracter_Agent\u001b[0m (to chat_manager):\n",
      "\n",
      "5\n",
      "25\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Adder_Agent\n",
      "\u001b[0m\n",
      "\u001b[33mAdder_Agent\u001b[0m (to chat_manager):\n",
      "\n",
      "6\n",
      "26\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "chat_result = number_agent.initiate_chat(\n",
    "    group_chat_manager,\n",
    "    message=\"My number is 3, I want to turn it into 13.\",\n",
    "    summary_method=\"reflection_with_llm\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chat summary\n",
    "\n",
    "The chat history shows each time an agent had to go to an LLM to complete at task. In our case, it was every time an operator agent was selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'My number is 3, I want to turn it into 13.', 'role': 'assistant'},\n",
      " {'content': '6', 'name': 'Multiplier_Agent', 'role': 'user'},\n",
      " {'content': '26', 'name': 'Multiplier_Agent', 'role': 'user'},\n",
      " {'content': '5\\n25', 'name': 'Subtracter_Agent', 'role': 'user'},\n",
      " {'content': '5\\n25', 'name': 'Subtracter_Agent', 'role': 'user'},\n",
      " {'content': '6\\n26', 'name': 'Adder_Agent', 'role': 'user'}]\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(chat_result.chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token consumption and costs\n",
    "\n",
    "Note that [AutoGen can cache LLM requests and responses](https://microsoft.github.io/autogen/docs/topics/llm-caching). It reduces the actual number of LLM calls.\n",
    "\n",
    "The cache is persistent. If you don't change the code, you should see only cached tokens used to complete the task. In addition, caching speeds up the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'usage_excluding_cached_inference': {'gpt-4-0613': {'completion_tokens': 77,\n",
      "                                                     'cost': 0.00762,\n",
      "                                                     'prompt_tokens': 100,\n",
      "                                                     'total_tokens': 177},\n",
      "                                      'total_cost': 0.00762},\n",
      " 'usage_including_cached_inference': {'gpt-4-0613': {'completion_tokens': 77,\n",
      "                                                     'cost': 0.00762,\n",
      "                                                     'prompt_tokens': 100,\n",
      "                                                     'total_tokens': 177},\n",
      "                                      'total_cost': 0.00762}}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(chat_result.cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is that a lot of tokens?\n",
    "\n",
    "Yes. For comparison, [Kennedy's \"we choose to go to the Moon\" speech](https://en.wikipedia.org/wiki/We_choose_to_go_to_the_Moon) is about 350 tokens. So, this example uses about half of that to do a little math.\n",
    "\n",
    "\n",
    "The famous part of the speech, for reference. It took him just over two minutes to deliver it.\n",
    "\n",
    "> We set sail on this new sea because there is new knowledge to be gained, and new rights to be won, and they must be won and used for the progress of all people. For space science, like nuclear science and all technology, has no conscience of its own. Whether it will become a force for good or ill depends on man, and only if the United States occupies a position of pre-eminence can we help decide whether this new ocean will be a sea of peace or a new terrifying theater of war. I do not say that we should or will go unprotected against the hostile misuse of space any more than we go unprotected against the hostile use of land or sea, but I do say that space can be explored and mastered without feeding the fires of war, without repeating the mistakes that man has made in extending his writ around this globe of ours. There is no strife, no prejudice, no national conflict in outer space as yet. Its hazards are hostile to us all. Its conquest deserves the best of all mankind, and its opportunity for peaceful cooperation may never come again. But why, some say, the Moon? Why choose this as our goal? And they may well ask, why climb the highest mountain? Why, 35 years ago, fly the Atlantic? Why does Rice play Texas? We choose to go to the Moon. We choose to go to the Moon... We choose to go to the Moon in this decade and do the other things, not because they are easy, but because they are hard; because that goal will serve to organize and measure the best of our energies and skills, because that challenge is one that we are willing to accept, one we are unwilling to postpone, and one we intend to win, and the others, too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
